{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd \n",
    "import os\n",
    "import google.cloud\n",
    "import pyodbc\n",
    "from google.cloud import storage\n",
    "import pyodbc\n",
    "import traceback\n",
    "import gcsfs\n",
    "import io\n",
    "import json\n",
    "from logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'transacciones-bancolombia-a6b73894e33f.json'\n",
    "client = storage.Client('canales-bancolombia-9dba0ee33646.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_config_json( conf_path ):\n",
    "    \"\"\"Carga un archivo json en un diccionario de python\n",
    "    recibe como parámetro la ruta de un archivo json\n",
    "    \"\"\"\n",
    "    if conf_path is None or conf_path == '':\n",
    "        raise RuntimeError('Archivo de Configuración no puede ser Nulo')\n",
    "    \n",
    "    with open( conf_path ) as f_in :\n",
    "        json_str = f_in.read()\n",
    "        return json.loads( json_str )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configuration(conf_path):\n",
    "    \"\"\"Devuelve un diccionario de python a partir de una ruta recibida por parámetros\n",
    "    Si no se envía un parámetro se genera un error.\n",
    "    \"\"\"\n",
    "    if len( conf_path  ) < 2:\n",
    "        raise RuntimeError(\"ERROR en Ejecución debe enviar la dirección del path del archivo de configuración\")\n",
    "    \n",
    "    return load_config_json(conf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_exception(ex,process,log):\n",
    "    \"\"\"Función Helper que permite hacer un stack de mensajes especificando el lugar de error en el código\n",
    "    La información obteniuda se escribe en el log como mensajes de error\n",
    "    \"\"\"\n",
    "    ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "    trace_back = traceback.extract_tb(ex_traceback)\n",
    "\n",
    "    # Format stacktrace\n",
    "    stack_trace = list()\n",
    "\n",
    "    for trace in trace_back:\n",
    "        stack_trace.append(\"File : %s , Line : %d, Func.Name : %s, Message : %s\" % (trace[0], trace[1], trace[2], trace[3]))\n",
    "    \n",
    "    log.Error('Error en {0}'.format(process))    \n",
    "    log.Error(\"Tipo de Excepcion : %s \" % ex_type.__name__)\n",
    "    log.Error(\"Mensaje : %s\" %ex_value)\n",
    "    log.Error(\"Stack trace : %s\" %stack_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storage_write(date,bucketName,bucketFolder,table,log):\n",
    "    try:\n",
    "        year = date.format('%Y')\n",
    "        month = date.format('%m')\n",
    "        day = date.format('%d')\n",
    "        bucket = client.get_bucket(bucketName)\n",
    "        filename = \"%s/%s\" % (str(bucketFolder) + '/2020',str(date) + '.csv')\n",
    "        bucket.blob(filename).upload_from_string(table.to_csv(index=False))\n",
    "        log.Info(\"Escribiendo fecha: \" + str(date) + \" en: \" + bucketName + \"/\" + bucketFolder)\n",
    "    except ValueError:\n",
    "        log.Error(\"No se completo la escritura en Google Cloud Storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para leer el último día en que se subio un archivo en google cloud storage y devuelve el o los días desde la ultima ingesta hasta la actual (t-1, tiempo actual menos un día)\n",
    "def listDatesfromUltimateIngestion(filename_in_bucket,log):\n",
    "    #Read from cloud storage\n",
    "    # get bucket with name\n",
    "    dth = datetime.date.today()\n",
    "    delta_ingestion = dth - datetime.timedelta(2)\n",
    "    delta_yesterday = dth - datetime.timedelta(1)\n",
    "    last_ingestion = delta_ingestion.strftime('%Y-%m-%d')\n",
    "    yesterday = delta_yesterday.strftime('%Y-%m-%d')\n",
    "    bucket = client.get_bucket('canales_bookmarks')\n",
    "    # get bucket data as blob\n",
    "    filename = \"%s\" % (filename_in_bucket + '.csv')\n",
    "    blob = bucket.get_blob(filename)\n",
    "    list_dates = pd.read_csv('gs://canales_bookmarks/'+ filename)\n",
    "    listdates = pd.DataFrame(list_dates, index=None)\n",
    "    max_date = listdates.max(axis=0)\n",
    "    if( max_date[0] == last_ingestion ):\n",
    "        datelist = pd.date_range(start = max_date[0],end = yesterday)\n",
    "        return datelist[1:]\n",
    "    elif( max_date[0] < last_ingestion ):\n",
    "        datelist = pd.date_range(start = max_date[0],end = yesterday)\n",
    "        return datelist[1:]\n",
    "    else:\n",
    "        log.Warning(\"El flujo de \" + filename_in_bucket + \" se encuentra actualizado\")\n",
    "        datelist = []\n",
    "        return datelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validarDatos(table,date,filename_for_fileingesttion,log):\n",
    "    canales_filtro = ['APP','SVP','SVE','SUC','CB','ALM']\n",
    "    validar_canales = table.isin(canales_filtro).any().any()\n",
    "    validar_archivo = table.empty\n",
    "    if not validar_archivo:\n",
    "        log.Info(\"El archivo contiene datos\")\n",
    "        if filename_for_fileingesttion != 'canales_np_td' and filename_for_fileingesttion != 'canales_np_tdc':\n",
    "            if  validar_canales:\n",
    "                log.Info(\"El archivo contiene los canales en la lista\")\n",
    "                log.Info(\"Seleccionando Bucket para escribir\")\n",
    "                storage_write(date,'canales_pruebas',filename_for_fileingesttion,table,log)\n",
    "                log.Info(\"Escribiendo ultima ejecucion\")\n",
    "                writeDatesInFile(date,filename_for_fileingesttion,log)\n",
    "                return True\n",
    "                log.Info(\"Archivo subido a Google Cloud storage\")        \n",
    "            else:\n",
    "                log.Warning(\"El archivo no contiene los canales en la lista\")\n",
    "                return False\n",
    "        else:\n",
    "            log.Info(\"Seleccionando Bucket para escribir\")\n",
    "            storage_write(date,'canales_pruebas',filename_for_fileingesttion,table,log)\n",
    "            log.Info(\"Escribiendo ultima ejecucion\")\n",
    "            writeDatesInFile(date,filename_for_fileingesttion,log)\n",
    "            return True \n",
    "    else:\n",
    "        log.Info(\"El archivo no contiene datos\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para cargar el archivo de fechas que suben a google cloud storage\n",
    "def writeDatesInFile(date,filename_for_fileingestion):\n",
    "    # Se obtiene la lista de las fechas desde la última fecha de carga hasta T-1(día actual -1)\n",
    "    last_ingestion = date\n",
    "    #datelist = pd.date_range( start = \"2020-01-01\" ,end = last_ingestion)\n",
    "    data_list = pd.DataFrame({'fecha_ultima_ejecucion':last_ingestion},index=[0])\n",
    "    #Se adiciona la fecha o las fechas a la lista de fechas cargadas y se validan para cada respectivo caso:\n",
    " \n",
    "    # en google cloud storage\n",
    "    bucket = client.get_bucket('canales_bookmarks')\n",
    "    filename = \"%s\" % (filename_for_fileingestion + '.csv') # Nombre de la Carpeta + Nombre del archivo.\n",
    "    bucket.blob(filename).upload_from_string(data_list.to_csv(index=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeEjecutionsInFile(filename_for_fileingestion,resultado):\n",
    "    date = time.strftime('%Y-%m-%d')\n",
    "    execution_time = time.strftime('%H:%M:%S')\n",
    "    bucket = client.get_bucket('canales_bookmarks')\n",
    "    filename = \"%s\" % (filename_for_fileingestion + '_ejecuciones.csv')\n",
    "    blob = bucket.get_blob(filename)\n",
    "    list_ejecutions = pd.read_csv('gs://canales_bookmarks/'+ filename)\n",
    "    new_ejecution = pd.DataFrame({'fecha_ultima_ejecucion':date,'hora de ejecucion':execution_time,'resultado':resultado},index=[0])\n",
    "    write_new = list_ejecutions.append(new_ejecution,sort=False)\n",
    "    bucket.blob(filename).upload_from_string(write_new.to_csv(index=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeDatesInFile('2020-05-20','canales_itc')\n",
    "writeDatesInFile('2020-05-20','canales_fisicos')\n",
    "writeDatesInFile('2020-05-05','canales_np_td')\n",
    "writeDatesInFile('2020-05-20','canales_np_tdc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeLog(bucketFolder,log,nameLog,logFile):\n",
    "    bucket = client.get_bucket('canales_logs')\n",
    "    filename = bucketFolder + \"/\" + nameLog + '.txt'\n",
    "    blob = bucket.blob(filename)\n",
    "    blob.upload_from_filename(logFile + filename)\n",
    "    log.Info(\"Escribiendo log \" + filename + \" en \" + str(bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(log,nameProcess,nameLog,logFile,max_tries=5):\n",
    "    tic = time.time()\n",
    "    configFile = get_configuration(\"configuracion.json\")\n",
    "    connStr = configFile[\"connStr\"]\n",
    "    for i in range(max_tries):\n",
    "        log.Info(\"Intento de conexion #\" + str(i) )\n",
    "        try:\n",
    "            time.sleep(3) \n",
    "            conn_string = connStr\n",
    "            conn = pyodbc.connect(conn_string, autocommit=True)\n",
    "            toc = time.time()\n",
    "            log.Info(\"Conexion Establecida \" + str(round(toc - tic)) + \" s\")\n",
    "            return conn\n",
    "            break\n",
    "        except pyodbc.Error as e:\n",
    "            sqlstate = e.args[1]\n",
    "            log_exception(e , \"Conexion con Impala\",log)\n",
    "            writeEjecutionsInFile(nameProcess,\"Fallido\")\n",
    "            sys.exit(\"Programa terminado con Error\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(nameProcess):\n",
    "    \"\"\"Función principal del sistema donde  se especifica las funciones a\n",
    "    ejecutar durante todo el proceso.\n",
    "        *  Construir parametros\n",
    "        *  Ejecutar consultas SQL\n",
    "        *  Otros procesos/métodos\n",
    "    \"\"\"\n",
    "    \n",
    "    configFile = get_configuration(\"configuracion.json\")\n",
    "    logFile = configFile[\"pathLog\"]\n",
    "    nameLog = time.strftime('%Y-%m-%d %H-%m-%S')\n",
    "    log = logger(pathlog = logFile + nameProcess + \"/\", logName = nameLog + \".txt\")\n",
    "    tic = time.time()\n",
    "    \n",
    "    log.Info(\"+-+-+-+-+-+-+- Inicio Proceso Principal ! +-+-+-+-+-+-+- \")\n",
    "    \n",
    "    cn = retry(log,nameProcess,nameLog,logFile)\n",
    "    success_list = []\n",
    "    error_list = []\n",
    "    log.Info(\"+-+-+-+-+-+-+- Inicio Proceso \" + nameProcess + \"! +-+-+-+-+-+-+- \")\n",
    "    try:\n",
    "        bucketFolder = nameProcess\n",
    "        file = open( nameProcess + '.sql', 'r')\n",
    "        sql = s = \" \".join(file.readlines())\n",
    "        datelist = listDatesfromUltimateIngestion(bucketFolder,log)\n",
    "        if len(datelist) > 0:\n",
    "            for date in datelist:\n",
    "                year = date.strftime('%Y')\n",
    "                month = date.strftime('%m')\n",
    "                day = date.strftime('%d')\n",
    "                date_time = date.strftime('%Y-%m-%d')\n",
    "                execution_time = time.strftime('%H:%M:%S')\n",
    "                log.Info(\"+-+-+-+-+-+-+- Inicio proceso para la fecha: \" + date_time + \"! +-+-+-+-+-+-+- \")\n",
    "                log.Info(\"Ejecutando Query, para la fecha:{0}-{1}-{2}\".format(year,month,day))\n",
    "                query_result = pd.read_sql(sql.format(year,month,day),cn)\n",
    "                log.Info(\"Consulta Terminada\")\n",
    "                log.Info('Numero de registros: ' + str(query_result.shape[0]))\n",
    "                log.Info('Registros Nulos:'+ str( query_result.isnull().sum()).replace('\\n','|').replace(' ',' '))\n",
    "                log.Info(\"Validando Datos\")\n",
    "                condition = validarDatos( query_result,date_time,bucketFolder,log)\n",
    "                log.Info(\"+-+-+-+-+-+-+- Final proceso para la fecha: \" + date_time + \"! +-+-+-+-+-+-+- \")\n",
    "                if not condition:\n",
    "                    log.Error(\"Ejecucion interrumpida, ha ocurrido un error en la fecha:\" + str(date_time))\n",
    "                    error_list.append(date_time)\n",
    "                    resultado = \"Fallido\"\n",
    "                    log.Info(\"+-+-+-+-+-+-+- Final proceso para la fecha: \" + date_time + \"! +-+-+-+-+-+-+- \")\n",
    "                    log.Info(\"############################################################################# \")\n",
    "                    break\n",
    "                else:\n",
    "                    success_list.append(date.strftime('%Y-%m-%d'))\n",
    "                    resultado = \"Exitoso\"            \n",
    "        else:\n",
    "            log.Error(\"Ejecucion de \" + nameProcess + \" Terminada\")\n",
    "            resultado = \"Actualizado\"\n",
    "            \n",
    "               \n",
    "    except Exception as e:\n",
    "        log_exception(e , \"Ejecución de \" + nameProcess + \" fallido\",log)\n",
    "        resultado = \"Fallido\"\n",
    "    \n",
    "    finally:\n",
    "        writeLog(bucketFolder,log,nameLog,logFile)\n",
    "        toc = time.time()\n",
    "        log.Info(\"Ejecuciones exitosas: \" +  str(success_list))\n",
    "        log.Info(\"Ejecuciones fallidas: \" +  str(error_list))\n",
    "        writeEjecutionsInFile(nameProcess,resultado)\n",
    "        log.Info(\"***** Fin de Ejecución del Proceso.  Duración Total(s): {0}\".format( round(toc - tic) ))\n",
    "        cn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('canales_itc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('canales_fisicos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('canales_np_td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('canales_np_tdc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
